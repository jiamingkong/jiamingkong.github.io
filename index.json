[{"content":" Random thoughts and recurrent ideas on LLMs.\n","date":"13 September 2024","externalUrl":null,"permalink":"/blogs/","section":"Blog Posts","summary":"\u003c!-- \n\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  Simple, yet powerful. Learn how to use Blowfish and its features.\n\u003c/div\u003e\n --\u003e\n\u003cp\u003eRandom thoughts and recurrent ideas on LLMs.\u003c/p\u003e\n\u003chr\u003e","title":"Blog Posts","type":"blogs"},{"content":"","date":"13 September 2024","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":" 1. Background # When ChatGPT first emerged, we wrote an article titled \u0026ldquo;Reflections on Transformers\u0026rdquo; to showcase our attempts. Over a year later, we have indeed participated in the development of new drugs and the understanding of pathology. Today, with the release of o1, we also want to demonstrate new attempts - teaching a large language model to reason, to tackle long-range, open-ended tasks.\nWe will first demonstrate the capabilities of our current model through a demo:\nProblem: Prove the lemma that n-th order polynomials on Z/p, where p is a prime, have at most n roots.\nModel prediction:\nPlan 1: Direct Proof via Polynomial Division\nAssume a polynomial has more than n roots. Choose n+1 distinct roots. Construct a polynomial of degree n using these roots (e.g., using the product of linear factors). Divide the original polynomial by this constructed polynomial. Show that the remainder must be zero, leading to a contradiction. proving the lemma via polynomial division:\nStep 1: Assume a polynomial has more than n roots.\nLet\u0026rsquo;s say we have a polynomial p(x) of degree n over Z/p, and assume it has n+1 distinct roots: r₁, r₂, \u0026hellip;, rₙ₊₁.\nStep 2: Construct a polynomial of degree n using these roots.\nWe can construct a polynomial q(x) of degree n using the product of linear factors corresponding to these roots:\nq(x) = (x - r₁) (x - r₂) \u0026hellip; (x - rₙ₊₁)\nStep 3: Divide the original polynomial by the constructed polynomial.\nPerform polynomial division of p(x) by q(x). Let the quotient be s(x) and the remainder be r(x):\np(x) = q(x) * s(x) + r(x)\nStep 4: Show that the remainder must be zero.\nSince each rᵢ is a root of p(x), we know p(rᵢ) = 0 for all i = 1 to n+1. Substituting x = rᵢ into the division equation, we get:\n0 = q(rᵢ) * s(rᵢ) + r(rᵢ)\nBecause q(x) is constructed to have roots at r₁, r₂, \u0026hellip;, rₙ₊₁, we have q(rᵢ) = 0 for all i. Therefore:\n0 = 0 * s(rᵢ) + r(rᵢ)\nThis implies r(rᵢ) = 0 for all i = 1 to n+1. Since the remainder r(x) is zero at n+1 distinct points, and the degree of r(x) cannot exceed n (it\u0026rsquo;s the remainder of a division where the divisor has degree n), the remainder r(x) must be the zero polynomial.\nConclusion:\nTherefore, p(x) = q(x) * s(x). This means p(x) is divisible by q(x), a polynomial of degree n. Since p(x) itself has degree n, it follows that p(x) cannot have more than n roots.\nThis answer appears to have all the necessary components. In this reasoning process, two models are used: 1) a larger \u0026ldquo;plan model\u0026rdquo; and 2) an average sized \u0026ldquo;execution model\u0026rdquo;. The first step involves the plan model providing CoT, \u0026ldquo;the plan\u0026rdquo;, followed by the execution model generating specific answers for subsequent steps.\nThe current reasoning topology differs from the web version of o1 in a noticeable way: the answers provided by the execution model (EM) are not verified by the plan model (PM). This means that even if our PM provides a perfect chain-of-thought (CoT) plan, the EM might still execute it incorrectly, and the error will be displayed to the user.\nOf course, this can be easily modified, providing a time and reasoning cost budget just like that of o1. Now, let\u0026rsquo;s replicate our own o1 instance in a slightly different way.\n2. Good Answers via More Compute # The way we interact with LLMs has undergone the following evolution:\nInitially, users directly issued tasks to the LLM:\nflowchart LR AV[User's Question]--\u003e CV((Execution Model)) --\u003e DV[Final Answer] Later, the concept of a thinking chain emerged, breaking down tasks into multiple steps for the model to complete:\nflowchart LR AR[User's Question]--\u003e|Think step by step!| CR((Execution Model)) --\u003e DR[Final Answer] Today, we see that by dynamically generating CoT, we can directly obtain better output results:\nflowchart LR AR[User's Question]--\u003e BR((Planner Model))--\u003e|CoT: Complete steps one through four to answer| CR((Execution Model)) --\u003e DR[Final Answer] Chain of Thoughts # The Chain of Thoughts (CoT) 1 is one of the most important inventions in the current field of LLM reasoning. A large problem can be broken down into multiple sub-problems with higher accuracy and easier-to-monitor intermediate processes through a CoT. This allows LLMs to solve the problem correctly.\nFor micro-tasks (generation length \u0026lt; 100), such as text classification, letting the LLM first identify relevant keywords and then summarize them into corresponding categories can significantly improve classification accuracy. For mid-level tasks (generation length \u0026lt; 1000), allowing the LLM to first explain how it will write code and then actually write it can also significantly improve code quality. Even for larger tasks (generation length \u0026gt; 1000), we have seen the emergence of intelligent agents through multiple chain interactions.\nCoT is effective, but writing high-quality CoT is challenging.\nFrom a grammatical perspective, CoT sentences resemble another syntax-less programming language. They are assembled from consecutive gerund phrases like \u0026ldquo;do A, calculate B, summarize into C\u0026rdquo;. Most language model vendors train a separate version of the model (often suffixed with \u0026ldquo;coder\u0026rdquo;) with the same number of parameters specifically for coding or mathematical tasks. This is partly due to the significant difference in token distribution.\nThe following example illustrates how our model, when answering open-ended questions, provides answers similar to those given by OpenAI\u0026rsquo;s o1 (indicating we are on the right track).\nQuestion: measuring extend lifespan could take a long time. how to test a new anti-aging drug in a faster way?\nRealm (codename) OpenAI o1 Focus on Cellular Senescence\n1. Isolate Senescent Cells: Culture cells from the target species and induce senescence using stressors (e.g., oxidative stress, radiation).\n2. Treat with Drug: Expose senescent cells to the anti-aging drug and monitor changes in senescence markers (e.g., beta-galactosidase activity, p16INK4a expression).\n3. Assess Cellular Function: Evaluate the drug\u0026rsquo;s impact on cellular functions affected by senescence, such as proliferation, DNA repair, and inflammatory cytokine production.\n4. Compare to Controls: Compare the drug-treated senescent cells to untreated controls and cells treated with known senolytics (drugs that eliminate senescent cells). \u0026hellip;Immune System Function: Assess immunosenescence by measuring responses to vaccines or infections.\nShort-Term Clinical Trials with Surrogate Endpoints\nHealthspan Indicators: Focus on quality-of-life measures rather than lifespan.\nDisease Progression Metrics: Track the impact on age-related diseases like diabetes, cardiovascular conditions, or osteoporosis.\nMetabolic Health: Monitor changes in glucose tolerance, lipid profiles, and insulin sensitivity. Semantically, CoT writing requires highly precise expression and an extremely broad knowledge base (e.g., the example of Stokes\u0026rsquo; theorem below). Currently, open-source agents rely on a series of manually refined prompts, which limits their accuracy in answering open-ended or long-range questions (questions whose answer would need a recursive thinking depth exceeding the pre-set prompt). The following example illustrates the difference in results obtained from the same open-source execution model when provided with a generic CoT versus a CoT written by Realm:\nQuestion: Prove \\(\\oint_{C}Pdx+Qdy=\\iint_D\\left( \\frac{\\partial Q}{\\partial x} -\\frac{\\partial P}{\\partial y}\\right)dxdy\\)\nRealm Equiv-param other LLMs Plan 1: Green\u0026rsquo;s Theorem as a Special Case1. Start with Stokes\u0026rsquo; Theorem: Recall Stokes\u0026rsquo; Theorem, which relates a line integral around a closed curve to a surface integral over the surface bounded by the curve.2. Choose a suitable surface: Let the surface be the region D itself, with the normal vector pointing upwards.3. Express Stokes\u0026rsquo; Theorem in 2D: Since D is a planar region, the curl of the vector field (P, Q, 0) simplifies to (∂Q/∂x - ∂P/∂y, 0, 0).4. Reduce to Green\u0026rsquo;s Theorem: The surface integral in Stokes\u0026rsquo; Theorem becomes the double integral of (∂Q/∂x - ∂P/∂y) over D, while the line integral reduces to the line integral in Green\u0026rsquo;s Theorem. \u0026hellip; Let (P(x,y)) and (Q(x,y)) be functions of (x) and (y). Apply Green\u0026rsquo;s Theorem: \\(\\oint_{C}Pdx+Qdy=\\iint_{D}\\left(\\frac{\\partial Q}{\\partial x}+\\frac{\\partial P}{\\partial y}\\right)dxdy\\), rearrange the equation and simplify into the desired form. Here we will present the execution result from Realm plan with an off-the-shelf llama 3.1. We didn\u0026rsquo;t include the comparison because the other plan was wrong to start with.\nStart with Stokes\u0026rsquo; Theorem: Stokes\u0026rsquo; Theorem states:\n$$ \\iint_S \\nabla \\times \\mathbf{F} \\cdot d\\mathbf{S} = \\oint_C \\mathbf{F} \\cdot d\\mathbf{r}$$\nwhere:\n$S$ is a smooth oriented surface bounded by a simple, closed, piecewise smooth curve $C$. $\\mathbf{F}$ is a continuously differentiable vector field. $\\nabla \\times \\mathbf{F}$ is the curl of $\\mathbf{F}$. $d\\mathbf{S}$ is the outward-pointing normal vector to the surface element. $d\\mathbf{r}$ is the tangent vector to the curve element. Choose a suitable surface: Let the surface $S$ be the region $D$ itself. Since $D$ is a planar region, we can choose the normal vector to point upwards (in the positive $z$ direction).\nExpress Stokes\u0026rsquo; Theorem in 2D: We need to find the curl of the vector field $\\mathbf{F} = (P, Q, 0)$. In 3D, the curl is given by:\n$$\\nabla \\times \\mathbf{F} = \\left( \\frac{\\partial R}{\\partial y} - \\frac{\\partial Q}{\\partial z}, \\frac{\\partial P}{\\partial z} - \\frac{\\partial R}{\\partial x}, \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right)$$\nSince $R = 0$, the curl simplifies to:\n$$\\nabla \\times \\mathbf{F} = \\left( 0, 0, \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right)$$\nTherefore, the surface integral in Stokes\u0026rsquo; Theorem becomes:\n$$\\iint_D \\left( \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right) dxdy$$\nReduce to Green\u0026rsquo;s Theorem: The line integral in Stokes\u0026rsquo; Theorem reduces to the line integral around the boundary curve $C$:\n$$\\oint_C \\mathbf{F} \\cdot d\\mathbf{r} = \\oint_C (Pdx + Qdy)$$\nTherefore, Stokes\u0026rsquo; Theorem in this case gives us:\n$$\\iint_D \\left( \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right) dxdy = \\oint_C (Pdx + Qdy)$$\nThis is precisely Green\u0026rsquo;s Theorem.\nIt\u0026rsquo;s clear that a tutor who can tell a student \u0026ldquo;You need to look into XYZ\u0026rdquo; is more likely to produce excellent students than one who can only say \u0026ldquo;You need to look into that.\u0026rdquo; These two examples effectively demonstrate the importance of a high-quality, specific CoT. We need to teach a model how to write one.\n3. Data Scarcity # High-quality CoT data is scarce. This scarcity stems from three main reasons:\nInternet data primarily reflects thinking outcomes, with intermediate processes and revisions already simplified or removed. Top scholars rarely upload scanned drafts of their papers when publishing.\nHigh-quality reasoning requires precise CoTs, not generic ones; CoTs that change with the problem, not static ones; and CoTs ranked by execution success probability, not simply sampled. It\u0026rsquo;s no longer a matter of a one-size-fits-all \u0026ldquo;Let\u0026rsquo;s think step by step\u0026rdquo; magic phrase. Generating CoT itself is an open problem.\nFor open problems with limited references necessitate synthetic data. A good synthetic solution requires the following cold-start elements:\nSeed data A generative workshop: Generator A quality control inspector: Discriminator 3.1 Seed Data # Our seed data is reverse-generated. Given a QA pair, we reflectively ask the model what thought process led to the answer and how that process could be abstracted to handle similar problems. This process can be executed well by any typical models. Additionally, since correct answers often mention keywords, the summarization model also incorporates this information (which is how our planner recognized the term \u0026ldquo;Stokes\u0026rdquo; when encountering the relevant formula, providing a significant jump start advantage to the execution model).\nThrough reflective questioning, further refinement, and final rephrasing, we obtain a series of seed CoTs. Since the answers in question-answer data are typically correct, we can assume that the seed data is of high quality.\nWe also employ direct evaluation methods to ensure the quality of our seed data. This involves:\nInputting the extracted CoTs into the model and executing them N times. Calculating the similarity and overlap between the generated answers and the actual QA answers, using Bertscore or BLEU for example. graph LR A[Question] B[Answer] A--\u003eC[“What thought process was demonstrated?”] B--\u003eC C --\u003e|Summarizing with the answer| D[\"Seed CoT\"] A --\u003e G[“How would you answer?”] G --\u003e|Direct Inference| F[Ref CoT] D --\u003e|Training| M[Discriminator] F --\u003e|Training| M 3.2 Discriminator # As shown in the diagram above, we can directly feed problems (without answers) to open-source models to attempt generating similar CoTs. These CoTs would likely be of lower quality. Now, for the same problem, we have both high-quality and low-quality CoTs, creating a preference dataset. I suspect many readers have already grasped the key steps of RLHF at this point.\n3.3 Generator # What we need is a quantity of high quality \u0026ldquo;deep\u0026rdquo; questions as the pseudo user input. We can extensively search for high-quality scientific papers or code snippets, extract chunks, and use the HyDE2 method to list potential questions related to those chunks. For example, given a passage explaining DNA sequencing methods, we could prompt the model to ask: \u0026ldquo;Since DNA has a double helix, how do we determine which strand each fragment belongs to during sequencing?\u0026rdquo; This approach allows us to generate a large number of high-quality questions.\n4. RL or SimPO # At this point, choosing between reinforcement learning or an equivalent method to train the model becomes a matter of preference. We opted for the SimPO method 3, whose basic workflow is as follows:\nThe Planner model to be fine-tuned receives two text strings as input: one consisting of the question plus a high-quality CoT, and the other consisting of the question plus a low-quality CoT. The average logits of the CoT portion are calculated. The difference in logits between the high-quality and low-quality CoTs is maximized. Due to the use of SimPO, we also need to sample new questions during training, generate new CoTs, and then use the Discriminator to evaluate and select the better ones. Looking back, it might be beneficial to train both the Discriminator and Generator online.\nOur training process is illustrated in the following diagram:\ngraph LR A[Sample Questions] --\u003e G((Generation Model)) G--\u003e|Use lemma1, do this| D((Discrimination Model)) G-.-\u003e|Let's think step by step| D((Discrimination Model)) D --\u003e GS[\\Good CoTs/] D -.-\u003e BS[\\Bad CoTs/] GS --\u003e SimPO(((SimPO Trainer))) BS -.-\u003e SimPO SimPO --\u003e|backprop| G 5. Reflections on the Implementation # Scaling by Compute: This approach opens up new possibilities. Previously, using LLMs for drafting and trial-and-error was limited by the user\u0026rsquo;s time spent breaking down tasks and evaluating results. Now, this limitation can be gradually overcome. Room for Improvement: This preview version is far from reaching the full potential of such models. By accumulating use case data, we can save well-executed CoTs as references for future use. Open search can also incorporate real-time data to influence the direction of CoT writing. References\n[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (arxiv.org)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n[2212.10496] Precise Zero-Shot Dense Retrieval without Relevance Labels (arxiv.org)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n[2405.14734] SimPO: Simple Preference Optimization with a Reference-Free Reward (arxiv.org)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"13 September 2024","externalUrl":null,"permalink":"/blogs/making_o1/","section":"Blog Posts","summary":"Re-implementing OpenAI O1\u0026rsquo;s active CoT","title":"Making a Strawberry in-house","type":"sample"},{"content":"","date":"13 September 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" ","date":"13 September 2024","externalUrl":null,"permalink":"/","section":"Welcome to Blowfish!","summary":"\u003c!-- \n\u003cdiv class=\"flex px-4 py-2 mb-8 text-base rounded-md bg-primary-100 dark:bg-primary-900\"\u003e\n  \u003cspan class=\"flex items-center ltr:pr-3 rtl:pl-3 text-primary-400\"\u003e\n    \n    \n      \u003cspan class=\"relative inline-block align-text-bottom icon\"\u003e\n        \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M506.3 417l-213.3-364c-16.33-28-57.54-28-73.98 0l-213.2 364C-10.59 444.9 9.849 480 42.74 480h426.6C502.1 480 522.6 445 506.3 417zM232 168c0-13.25 10.75-24 24-24S280 154.8 280 168v128c0 13.25-10.75 24-23.1 24S232 309.3 232 296V168zM256 416c-17.36 0-31.44-14.08-31.44-31.44c0-17.36 14.07-31.44 31.44-31.44s31.44 14.08 31.44 31.44C287.4 401.9 273.4 416 256 416z\"/\u003e\u003c/svg\u003e\n    \n      \u003c/span\u003e\n    \n\n  \u003c/span\u003e\n  \u003cspan class=\"flex items-center justify-between grow dark:text-neutral-300\"\u003e\n    \u003cspan class=\"prose dark:prose-invert\"\u003eThis is a demo of the \u003ccode id=\"layout\"\u003ebackground\u003c/code\u003e layout.\u003c/span\u003e\n    \u003cbutton\n      id=\"switch-layout-button\"\n      class=\"px-4 !text-neutral !no-underline rounded-md bg-primary-600 hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700\"\n    \u003e\n      Switch layout \u0026orarr;\n    \u003c/button\u003e\n  \u003c/span\u003e\n\u003c/div\u003e --\u003e\n\u003c!-- \u003clite-youtube videoid=\"SgXhGb-7QbU\" playlabel=\"SgXhGb-7QbU\" params=\"\"\u003e\u003c/lite-youtube\u003e\n --\u003e","title":"Welcome to Blowfish!","type":"page"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Un rapido esempio di come iniziare a utilizzare le tassonomie degli autori nei tuoi articoli.\n","externalUrl":null,"permalink":"/authors/_index.it/","section":"Jiaming Kong","summary":"\u003cp\u003eUn rapido esempio di come iniziare a utilizzare le tassonomie degli autori nei tuoi articoli.\u003c/p\u003e","title":"Esempio di elenco di tassonomia degli autori","type":"authors"},{"content":"Hi, my name is Jiaming Kong, I work on RWKV and speech recognition models.\n","externalUrl":null,"permalink":"/authors/","section":"Jiaming Kong","summary":"\u003cp\u003eHi, my name is Jiaming Kong, I work on RWKV and speech recognition models.\u003c/p\u003e","title":"Jiaming Kong","type":"authors"},{"content":"I work on RWKV.\n","externalUrl":null,"permalink":"/authors/jiamingkong/","section":"Jiaming Kong","summary":"\u003cp\u003eI work on RWKV.\u003c/p\u003e","title":"Jiaming Kong","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"あなたの記事でどのように著者の分類を開始するかの簡単な例です。\n","externalUrl":null,"permalink":"/authors/_index.ja/","section":"Jiaming Kong","summary":"\u003cp\u003eあなたの記事でどのように著者の分類を開始するかの簡単な例です。\u003c/p\u003e","title":"著者の分類リストの例","type":"authors"}]