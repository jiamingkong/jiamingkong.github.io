[{"content":"","date":"2024-09-13","externalUrl":null,"permalink":"/zh-cn/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"2024-09-13","externalUrl":null,"permalink":"/zh-cn/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"关于大语言模型的一些不成形的思考\n","date":"2024-09-13","externalUrl":null,"permalink":"/zh-cn/blogs/","section":"博客","summary":"\u003cp\u003e关于大语言模型的一些不成形的思考\u003c/p\u003e\n\u003chr\u003e","title":"博客","type":"blogs"},{"content":" 这是 background 的样式示例。 切换 layout \u0026orarr; npx blowfish-tools ","date":"2024-09-13","externalUrl":null,"permalink":"/zh-cn/","section":"欢迎来到 Blowfish! 🎉","summary":"\u003cdiv class=\"flex px-4 py-2 mb-8 text-base rounded-md bg-primary-100 dark:bg-primary-900\"\u003e\n  \u003cspan class=\"flex items-center ltr:pr-3 rtl:pl-3 text-primary-400\"\u003e\n    \n    \n      \u003cspan class=\"relative inline-block align-text-bottom icon\"\u003e\n        \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M506.3 417l-213.3-364c-16.33-28-57.54-28-73.98 0l-213.2 364C-10.59 444.9 9.849 480 42.74 480h426.6C502.1 480 522.6 445 506.3 417zM232 168c0-13.25 10.75-24 24-24S280 154.8 280 168v128c0 13.25-10.75 24-23.1 24S232 309.3 232 296V168zM256 416c-17.36 0-31.44-14.08-31.44-31.44c0-17.36 14.07-31.44 31.44-31.44s31.44 14.08 31.44 31.44C287.4 401.9 273.4 416 256 416z\"/\u003e\u003c/svg\u003e\n    \n      \u003c/span\u003e\n    \n\n  \u003c/span\u003e\n  \u003cspan class=\"flex items-center justify-between grow dark:text-neutral-300\"\u003e\n    \u003cspan class=\"prose dark:prose-invert\"\u003e 这是 \u003ccode id=\"layout\"\u003ebackground\u003c/code\u003e 的样式示例。\u003c/span\u003e\n    \u003cbutton\n      id=\"switch-layout-button\"\n      class=\"px-4 !text-neutral !no-underline rounded-md bg-primary-600 hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700\"\n    \u003e\n      切换 layout \u0026orarr;\n    \u003c/button\u003e\n  \u003c/span\u003e\n\u003c/div\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-node\" data-lang=\"node\"\u003enpx blowfish-tools\n\u003c/code\u003e\u003c/pre\u003e\u003clite-youtube videoid=\"SgXhGb-7QbU\" playlabel=\"SgXhGb-7QbU\" params=\"\"\u003e\u003c/lite-youtube\u003e","title":"欢迎来到 Blowfish! 🎉","type":"page"},{"content":" Background # 之前在chatGPT横空出世的时候，我们写了个文章《Reflections on Transformers》，展示了我们的尝试。时隔一年多，我们确实也参与到了一些新药的研发、病理的理解中。今天o1出来了，我们也想展示一下新的尝试——让一个大语言模型学会推理，以面对长程、开放式的任务。我们先demo演示当前模型的能力：\n问题： prove the lemma that n-th order polynomials on Z/p, where p is a prime, has at most n roots.\n模型答案：\nPlan 1: Direct Proof via Polynomial Division\nAssume a polynomial has more than n roots. Choose n+1 distinct roots. Construct a polynomial of degree n using these roots (e.g., using the product of linear factors). Divide the original polynomial by this constructed polynomial. Show that the remainder must be zero, leading to a contradiction. proving the lemma via polynomial division:\nStep 1: Assume a polynomial has more than n roots.\nLet\u0026rsquo;s say we have a polynomial p(x) of degree n over Z/p, and assume it has n+1 distinct roots: r₁, r₂, \u0026hellip;, rₙ₊₁.\nStep 2: Construct a polynomial of degree n using these roots.\nWe can construct a polynomial q(x) of degree n using the product of linear factors corresponding to these roots:\nq(x) = (x - r₁) (x - r₂) \u0026hellip; (x - rₙ₊₁)\nStep 3: Divide the original polynomial by the constructed polynomial.\nPerform polynomial division of p(x) by q(x). Let the quotient be s(x) and the remainder be r(x):\np(x) = q(x) * s(x) + r(x)\nStep 4: Show that the remainder must be zero.\nSince each rᵢ is a root of p(x), we know p(rᵢ) = 0 for all i = 1 to n+1. Substituting x = rᵢ into the division equation, we get:\n0 = q(rᵢ) * s(rᵢ) + r(rᵢ)\nBecause q(x) is constructed to have roots at r₁, r₂, \u0026hellip;, rₙ₊₁, we have q(rᵢ) = 0 for all i. Therefore:\n0 = 0 * s(rᵢ) + r(rᵢ)\nThis implies r(rᵢ) = 0 for all i = 1 to n+1. Since the remainder r(x) is zero at n+1 distinct points, and the degree of r(x) cannot exceed n (it\u0026rsquo;s the remainder of a division where the divisor has degree n), the remainder r(x) must be the zero polynomial.\nConclusion:\nTherefore, p(x) = q(x) * s(x). This means p(x) is divisible by q(x), a polynomial of degree n. Since p(x) itself has degree n, it follows that p(x) cannot have more than n roots.\n这看上去算是该有的都有，在这个推理中，两个模型被使用：1）一个更大体积的plan model，2）一个常规大小的execution model。第一步是plan model给出的指引，后续的步骤是execution model给出的具体的答案。当前我们的推理拓扑与网页版o1有一个比较明显的差异就是：EM给出的答案不会被PM验真，即当前的版本中就算我们的PM给出完美的CoT plan，EM也可能执行错，并且展示在用户面前。当然这件事比较容易修改，但相应的，我们也要给出时间和推理成本的预算。现在我们来以一个不太一样的方式重现一个o1。\nGood answers via more compute # 我们调用LLM的模式已经进过了如下历程改变，最早的时候用户直接向LLM下达任务：\nflowchart LR AV[用户的问题]--\u003e CV((Execution Model)) --\u003e DV[最终答案] 后来有了思维链，任务被拆解成多个环节让模型完成：\nflowchart LR AR[用户的问题]--\u003e|Think step by step!| CR((Execution Model)) --\u003e DR[最终答案] 到今天，我们可以看到，通过将CoT动态化，我们可以直接得到更好的输出结果：\nflowchart LR AR[用户的问题]--\u003e BR((Planner Model))--\u003e|CoT:完成一二三四以解答| CR((Execution Model)) --\u003e DR[最终答案] Chain of thoughts # 逻辑链（Chain of Thoughts）1 是整个LLM在当前推理业务中最重要的发明之一，一个大型问题通过思维链可以拆解成多个正确率高、中间过程更容易监控的子问题，从而被LLM正确求解。在微观任务上(generation length \u0026lt; 100)，以文本分类为例，让LLM先找到相关的字眼，再总结为对应的分类，可以显著地提升文本分类的正确率；在中层任务上(generation length \u0026lt; 1000)，让LLM先说明它会如何写一个代码，再实际地上手写，也可以显著地提升代码质量。在更大型的任务上(generation length \u0026gt; 1000)，通过多个链条的交互，我们也见到了智能体的涌现。\nCoT是有效的，但写出高质量的CoT是困难的。\n从语法的层面来说，CoT的句子接近另一种编程语言，它是连续的动名词短句的组装，do A, calculate B, summarize into C，这个语言只是没有明确的语法。当前的大多数语言模型厂商都会把编程的模型（甚至数学模型）单独训练一个同样参数量的版本出来，后缀为coder，有一部分考虑就是token分布的巨大差异。以下面的例子来说明，我们这个模型解答开放性的问题时，和openAI的o1给出的答案也是接近的（我们一定做对了什么）。\n问题：measuring extend lifespan could take a long time. how to test a new anti-aging drug in a faster way?\nRealm OpenAI o1 Focus on Cellular Senescence\n1. Isolate Senescent Cells: Culture cells from the target species and induce senescence using stressors (e.g., oxidative stress, radiation).\n2. Treat with Drug: Expose senescent cells to the anti-aging drug and monitor changes in senescence markers (e.g., beta-galactosidase activity, p16INK4a expression).\n3. Assess Cellular Function: Evaluate the drug\u0026rsquo;s impact on cellular functions affected by senescence, such as proliferation, DNA repair, and inflammatory cytokine production.\n4. Compare to Controls: Compare the drug-treated senescent cells to untreated controls and cells treated with known senolytics (drugs that eliminate senescent cells). Immune System Function: Assess immunosenescence by measuring responses to vaccines or infections.\nShort-Term Clinical Trials with Surrogate Endpoints\nHealthspan Indicators: Focus on quality-of-life measures rather than lifespan.\nDisease Progression Metrics: Track the impact on age-related diseases like diabetes, cardiovascular conditions, or osteoporosis.\nMetabolic Health: Monitor changes in glucose tolerance, lipid profiles, and insulin sensitivity. 从文义的层面来说，CoT的撰写则需要高度精确的表达，以及极度广阔的知识面（例如下面这个的Stokes引理的例子）。当前开源的agents都基于一系列手工打磨的prompt工作，这也局限了agents针对开放问题，或者长程问题（递归深度超过了预设prompt的问题）的解答精度。以下面的例子说明，当我们给定一个泛泛而谈的CoT，以及一个Realm写出来的CoT，在同样的开源的执行模型中得到的差异：\n问题：Prove \\(\\oint_{C}Pdx+Qdy=\\iint_D\\left( \\frac{\\partial Q}{\\partial x} -\\frac{\\partial P}{\\partial y}\\right)dxdy\\)\nRealm 同参量的其他模型 CoT generated Plan 1: Green\u0026rsquo;s Theorem as a Special Case1. Start with Stokes\u0026rsquo; Theorem: Recall Stokes\u0026rsquo; Theorem, which relates a line integral around a closed curve to a surface integral over the surface bounded by the curve.2. Choose a suitable surface: Let the surface be the region D itself, with the normal vector pointing upwards.3. Express Stokes\u0026rsquo; Theorem in 2D: Since D is a planar region, the curl of the vector field (P, Q, 0) simplifies to (∂Q/∂x - ∂P/∂y, 0, 0).4. Reduce to Green\u0026rsquo;s Theorem: The surface integral in Stokes\u0026rsquo; Theorem becomes the double integral of (∂Q/∂x - ∂P/∂y) over D, while the line integral reduces to the line integral in Green\u0026rsquo;s Theorem. Let (P(x,y)) and (Q(x,y)) be functions of (x) and (y). Apply Green\u0026rsquo;s Theorem: \\(\\oint_{C}Pdx+Qdy=\\iint_{D}\\left(\\frac{\\partial Q}{\\partial x}+\\frac{\\partial P}{\\partial y}\\right)dxdy\\), Rearrange the equation and simplify into the desired form. 在这里我们展示上述Realm plan被开源的普通模型执行的结果，另一侧对比的就不放上来了，因为是错的。\nStart with Stokes\u0026rsquo; Theorem: Stokes\u0026rsquo; Theorem states:\n$$ \\iint_S \\nabla \\times \\mathbf{F} \\cdot d\\mathbf{S} = \\oint_C \\mathbf{F} \\cdot d\\mathbf{r}$$\nwhere:\n$S$ is a smooth oriented surface bounded by a simple, closed, piecewise smooth curve $C$. $\\mathbf{F}$ is a continuously differentiable vector field. $\\nabla \\times \\mathbf{F}$ is the curl of $\\mathbf{F}$. $d\\mathbf{S}$ is the outward-pointing normal vector to the surface element. $d\\mathbf{r}$ is the tangent vector to the curve element. Choose a suitable surface: Let the surface $S$ be the region $D$ itself. Since $D$ is a planar region, we can choose the normal vector to point upwards (in the positive $z$ direction).\nExpress Stokes\u0026rsquo; Theorem in 2D: We need to find the curl of the vector field $\\mathbf{F} = (P, Q, 0)$. In 3D, the curl is given by:\n$$\\nabla \\times \\mathbf{F} = \\left( \\frac{\\partial R}{\\partial y} - \\frac{\\partial Q}{\\partial z}, \\frac{\\partial P}{\\partial z} - \\frac{\\partial R}{\\partial x}, \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right)$$\nSince $R = 0$, the curl simplifies to:\n$$\\nabla \\times \\mathbf{F} = \\left( 0, 0, \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right)$$\nTherefore, the surface integral in Stokes\u0026rsquo; Theorem becomes:\n$$\\iint_D \\left( \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right) dxdy$$\nReduce to Green\u0026rsquo;s Theorem: The line integral in Stokes\u0026rsquo; Theorem reduces to the line integral around the boundary curve $C$:\n$$\\oint_C \\mathbf{F} \\cdot d\\mathbf{r} = \\oint_C (Pdx + Qdy)$$\nTherefore, Stokes\u0026rsquo; Theorem in this case gives us:\n$$\\iint_D \\left( \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right) dxdy = \\oint_C (Pdx + Qdy)$$\nThis is precisely Green\u0026rsquo;s Theorem.\n很明显，一个能告诉学生“你得查一下XYZ”的导师，比起只能说“你得查一下”的导师更有可能带出优秀的学生。以上两个例子可以充分说明一个高质量的、具体化的CoT的重要性，我们需要教会一个模型写出来。\nData scarsity # 高质量的CoT数据是稀少的。这个稀少有三方面原因：\n我们在互联网语料上看到的都是思维结果，中间过程和修改过程都已经被化简删改。顶尖学者发表论文的时候并不会把草稿纸也同步扫描上传。 高质量的reasoning需要的CoT是精确的，不是通用的；是随问题改变的，不是静态的；是按照执行成功概率排序过的，不是简单采样生成的。不再是\u0026quot;Let\u0026rsquo;s think step by step\u0026quot;的一句话魔法了。即：生成CoT本身也是一个开放问题。 开放问题，极少参考，我们需要借助合成数据的力量了。一个良好的合成方案需要以下的冷启动要素： 种子数据 一个生成的车间 Generator 一个评价的质检员 Discriminator Seed data # 我们的种子数据是反向生成的，即，给定了QA对，我们会以反思的方式去询问模型，刚才这个答案展示了什么样的思考过程，这个过程能如何抽象以应对这类问题。这个过程在普通的模型中也能执行得很好。同时，因为正确的答案中通常提到了关键字眼，所以总结模型也会带入这类信息（这也是我们的planner是怎么见到这类公式就写出了Stokes这个词，给执行模型提供了巨大的便利）。\n反思询问，如有需要，进一步提炼，最后rephrase，就得到了一系列的种子CoT。既然问答数据里的答案通常是对的，我们可以认为种子数据都是优秀数据。\n我们也用了直接评估的方法对种子数据进行了质量把控，其方法如下：\n提取出来的CoT会输入到模型中，执行N次 计算这些答案和实际的QA答案的相似度、重合度等。 graph LR A[问题] B[答案] A--\u003eC[“展示了什么思维过程”] B--\u003eC C --\u003e|带着答案总结| D[\"Seed CoT\"] A --\u003e G[“你会怎么解答”] G --\u003e|直接推理| F[Ref CoT] D --\u003e|训练| M[Discriminator] F --\u003e|训练| M Discriminator # 如上图所示，我们可以直接将问题（不需要答案）扔给开源的模型们去尝试给出类似的CoT，我们可以认为这里面的CoT都是比较糟糕的。这时候对于同一个问题，我们就得到了优秀的答案和劣质的答案，这是一个偏好数据。我猜很多人读到这里已经理解到了RLHF的关键步骤已经完成了。\nGenerator # 我们可以大量地找高质量的科学论文或者代码段，取出chunk，按照HyDE2的方法，列举出这些chunk里可以问的问题。例如，给它一个介绍DNA测序手段的文段，让模型发问：“既然DNA有双链，那测序的时候我怎么知道每个碎片是哪条链的”。这样，我们也能得到大量的优秀的问题。\nRL or SimPO # 到这个位置，选择强化学习的方式、或者其他等效方式来训练模型就变成了一个口味选择了。我们采用的是SimPO方法3 ，其基础流程是这样的：\n对要微调的Planner模型输入两个文本串，一个是问题加上优质CoT，一个是问题加上劣质的CoT。 对CoT部分计算平均的logits 最大化优质CoT和劣质CoT的logits差异。 因为使用SimPO，所以也要求我们在训练的过程中采样新的问题，生成新的CoT，再用Discriminator进行评估选出里面较好的部分。现在回头看，可能Discriminator和Generator都应该是online训练的，会更好。\n总结，我们的训练过程如下图：\ngraph TB A[Sample Questions] --\u003e G((生成模型)) G--\u003e|Use lemma1, do this| D((辨别模型)) G-.-\u003e|Let's think step by step| D((辨别模型)) D --\u003e GS[\\Good CoTs/] D -.-\u003e BS[\\Bad CoTs/] GS --\u003e SimPO(((SimPO Trainer))) BS -.-\u003e SimPO SimPO --\u003e|backprop| G 复现感想 # Scale by compute是一个新的路，之前，用LLM做草稿试错，成本局限在用户拆解任务和评判的时间，那现在这个局限也可以被逐步打破。 当前的这个预览版远远没到这类模型该有的水平，通过积累数据，我们其实可以将“执行得好的CoT保存作为后续的参考”，开放search将实时数据加入，影响CoT的书写方向。 References\n[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (arxiv.org)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n[2212.10496] Precise Zero-Shot Dense Retrieval without Relevance Labels (arxiv.org)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n[2405.14734] SimPO: Simple Preference Optimization with a Reference-Free Reward (arxiv.org)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-09-13","externalUrl":null,"permalink":"/zh-cn/blogs/making_o1/","section":"博客","summary":"复现OpenAI O1的动态思维链","title":"种一颗小草莓","type":"sample"},{"content":"","externalUrl":null,"permalink":"/zh-cn/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Un rapido esempio di come iniziare a utilizzare le tassonomie degli autori nei tuoi articoli.\n","externalUrl":null,"permalink":"/authors/_index.it/","section":"Jiaming Kong","summary":"\u003cp\u003eUn rapido esempio di come iniziare a utilizzare le tassonomie degli autori nei tuoi articoli.\u003c/p\u003e","title":"Esempio di elenco di tassonomia degli autori","type":"authors"},{"content":"我是孔嘉明，我主要研究 RWKV 和语音识别模型。这里是我记录的一些想法和项目。\n","externalUrl":null,"permalink":"/zh-cn/authors/","section":"Jiaming Kong","summary":"\u003cp\u003e我是孔嘉明，我主要研究 RWKV 和语音识别模型。这里是我记录的一些想法和项目。\u003c/p\u003e","title":"Jiaming Kong","type":"authors"},{"content":"I work on RWKV.\n","externalUrl":null,"permalink":"/authors/jiamingkong/","section":"Jiaming Kong","summary":"\u003cp\u003eI work on RWKV.\u003c/p\u003e","title":"Jiaming Kong","type":"authors"},{"content":"","externalUrl":null,"permalink":"/zh-cn/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"あなたの記事でどのように著者の分類を開始するかの簡単な例です。\n","externalUrl":null,"permalink":"/authors/_index.ja/","section":"Jiaming Kong","summary":"\u003cp\u003eあなたの記事でどのように著者の分類を開始するかの簡単な例です。\u003c/p\u003e","title":"著者の分類リストの例","type":"authors"}]